---
title: 'The Federalist Papers: Author Classification'
output:
  html_document:
    df_print: paged
---

**The documents were formatted in three different ways. In the first way, minimal changes were made to the text. Symbols were removed and phrases that appeared in all documents like "To the People of the State of New York" and "PUBLIUS" were taken out. The second way took out all numbers, converted the text to lowercase, and took out stop words. Finally, the last formatting took out several topic related words and took out numbers. Some of this processing is shown in the code below.** 

**Trigrams generated from text data with minor formatting (trigram code included in Python script)** 

**Hamilton:**
**The force of habit . And indeed , but they can not extend further than to have an equal distance to subdue a part . if we also take into view the co-operation of its authority which is established by their own judges . the pretense of a week , a month , a new government it would require a State appears to have recourse to . As their requisitions , unable by its situation must have regulated the conduct of foreign negotiations , the reputation , and the seditions of whole classes of causes arising under those constitutions ,**

**Madison:**
**Constitution has provided the most numerous branch of the peace and sheriffs ; and that these expressions are absolutely irreconcilable to each a constitutional control over , the officers , it is declared `` that the same cause , and to each a constitutional augmentation of the innovation ? do they require that , or passages , without desiring , or a minority of citizens ? and what are many of them ; that it can , in like manner , by means on the first object of **

**Jay:** 
**The case of disputes what umpire shall decide between them and with Britain adds great weight to this plan , as well from the variety of useful information . that , as well as dazzle . if the English militia obeyed the government of England , if we are in reflecting that they are actuated by mercenary or friendly motives ; and should any circumstance occur which requires the advice and consent of both was essential to their being `` joined in affection and free people . But in such hopes . It will not admit that the president must ** 


```{r}
library(MASS)
```

### Format Data: 


```{r, warning=FALSE}
# read in data
setwd("~/Desktop")

# read in authors 
authors <- read.csv("authors.csv")
a <- authors[,2] # make it a vector 
single <- a %in% c("Hamilton", "Madison", "Jay") # pull out indices of documents that have a single author
b <- as.factor(as.character(a[single])) # names of those authors

x <- read.csv("WordFreq_NormalText.csv",row.names = 1)
# center data 
xbar <- apply(x, 2, mean)
for(i in 1:nrow(x)) {
  x[i, ] <- x[i, ] - xbar
}
x[is.na(x)] <- 0
y <- x[single, ] # get docs attributed to single author 


x2 <- read.csv("WordFreq_TopicWords.csv",row.names = 1) # read in modified data, this data has numbers and topic related words removed
xbar <- apply(x2, 2, mean)
for(i in 1:nrow(x2)) {
  x2[i, ] <- x2[i, ] - xbar
}
x2[is.na(x2)] <- 0
y2 <- x2[single, ]


x3 <- read.csv("WordFreq_StopWords.csv",row.names = 1)  # this data has numbers and stop words removed, everything is converted to lower case 
xbar <- apply(x3, 2, mean)
for(i in 1:nrow(x3)) {
  x3[i, ] <- x3[i, ] - xbar
}
x3[is.na(x3)] <- 0
y3 <- x3[single, ]
```


### PCA Without Sclaing 
```{r}
# PCA without scaling
p <- prcomp(y)
#head(p$rotation) 

# color by author 
plot(as.matrix(y) %*% p$rotation[, 1:2], col=b,pch = 16, main = "PCA (Without Scaling)")
legend("topright", levels(b), text.col=1:length(levels(b)),
       fill=1:length(levels(b)))
```
**This plot shows the federalist papers spread out based on word frequencies -- the authorship of each of the papers was not taken into consideration when plotting these points. This is interesting because we can see already that the data points seem to form general clusters based on the author. Although more analysis has to be done, this is a promising sign that the authors do indeed use different word frequencies that will possible be distinguishable.**


```{r}
# PCA without scaling
p <- prcomp(y3)

# color this by author 
plot(as.matrix(y3) %*% p$rotation[, 1:2], col=b,pch = 16, main = "PCA Without Scaling \nStop Words Removed")
legend("topright", levels(b), text.col=1:length(levels(b)),
       fill=1:length(levels(b)))
```
**The results look much different when PCA is conducted on the modified text. The modified text has numbers and stop words removed. Additionally, the words are all converted to lower case. When this formatting is done, we do not see the same groupings that we did in the example above. This perhaps indicates that things like stopwords and capitalization are important in distinguishing the authors. For our analysis, it may be useful to leave them in. **

```{r}
# PCA without scaling
p <- prcomp(y2)

# color this by author 
plot(as.matrix(y2) %*% p$rotation[, 1:2], col=b,pch = 16, main = "PCA Without Scaling \n Topic Words Removed")
legend("topright", levels(b), text.col=1:length(levels(b)),
       fill=1:length(levels(b)))
```

**This text was modified by taking out numbers and topic specific words including Constitution, President, Senate, House of Representatives, United States, America, federal, etc. Stop words were not removed and capitalization was kept as is. Again, we can already see some initial separation of the groups by their authors even though the data points were separated based on word frequencies, not by author.**


### PCA With Sclaing 

```{r}
absent <- which(apply(y, 2, var)==0) # which variables have 0 variance in y
z <- y[, -absent] # words that didn't show up in single known author docs -- removing these columns 
p2 <- prcomp(z, scale=TRUE)

plot(as.matrix(z) %*% p2$rotation[, 1:2], col=b,pch=16, main = "PCA With Scaling")
legend("bottomleft", levels(b), text.col=1:length(levels(b)),
       fill=1:length(levels(b)))
```
**In this plot, we do not seem to see the clusters of the authors like we did in some of the plots above. The plot of PCA with scaling looks decently different than the plot of PCA without scaling. It is not clear yet which will perform better in the predictive model, but this does seem to indicate that scaling may alter the performance.**


```{r}
absent <- which(apply(y2, 2, var)==0) # which variables have 0 variance in y
z <- y2[, -absent] # words that didn't show up in single known author docs -- removing these columns 
p2 <- prcomp(z, scale=TRUE)

plot(as.matrix(z) %*% p2$rotation[, 1:2], col=b,pch=16, main = "PCA With Scaling \n Topic Related Words Removed")
legend("bottomleft", levels(b), text.col=1:length(levels(b)),
       fill=1:length(levels(b)))
```

```{r}
absent <- which(apply(y3, 2, var)==0) # which variables have 0 variance in y
z <- y3[, -absent] # words that didn't show up in single known author docs -- removing these columns 
p2 <- prcomp(z, scale=TRUE)

plot(as.matrix(z) %*% p2$rotation[, 1:2], col=b,pch=16, main = "PCA With Scaling \n Stop Words Removed")
legend("bottomleft", levels(b), text.col=1:length(levels(b)),
       fill=1:length(levels(b)))
```
**Interestingly, the PCA plot with stop words removed now looks very similar to the PCA plot with topic related words removed. Noticeably, in all three examples shown above, some of Hamilton's papers appear to be very far outside the distribution of all of the other papers. It is unclear right now why these points are outliers.**


### LDA: 

```{r}
# PCA with different dimensions: 
p <- prcomp(y)

w40 <- as.matrix(y) %*% p$rotation[,1:40] 
l40 <- lda(w40, grouping=b)

w50 <- as.matrix(y) %*% p$rotation[,1:50] 
l50 <- lda(w50, grouping=b)

w60 <- as.matrix(y) %*% p$rotation[,1:60] 
l60 <- lda(w60, grouping=b)
```


```{r}
# Make a null lineup to compare observed separation of the documents with null results
par(mfrow = c(3, 3))
answer <- sample(1:9, 1)
for (i in 1:9) {
  if (i == answer) {
    # LDA for the original data
    plot(as.matrix(w40) %*% l40$scaling[, 1:2], col=b)
  } else {
    # LDA with labels reassigned at random
    B <- sample(b, length(b))
    L <- lda(w40, grouping=B)
    plot(as.matrix(w40) %*% L$scaling[, 1:2], col=B,pch = 16)
  }
}
par(mfrow = c(1, 1))
```

```{r}
par(mfrow = c(3, 3))
answer <- sample(1:9, 1)
for (i in 1:9) {
  if (i == answer) {
    # LDA for the original data
    plot(as.matrix(w60) %*% l60$scaling[, 1:2], col=b)
  } else {
    # LDA with labels reassigned at random
    B <- sample(b, length(b))
    L <- lda(w60, grouping=B)
    plot(as.matrix(w60) %*% L$scaling[, 1:2], col=B,pch=16)
  }
}
par(mfrow = c(1, 1))
```

**Above I have shown the null lineups of observed separation compared to documents will null results after performing PCA with a various numbers of principal components -- shown above are the examples for 40 and 60 principal components. The graph with the open dots is the graph for our actual data, while the graph will the filled point are the null results. In the plots that correspond to the example where 40 principal components were used, it is very easy to pick out the true data among the null results. If authors and word frequencies were not related, we would expect the LDA plot from the actual data to look like the null results. However, since the LDA plot of the actual data does stand out from the null results, this gives us evidence that authors are indeed related to word frequencies.**

**However, in the example where 60 principal components are used we also see separation based on author in the null results. This indicates that the results are likely being artificially spread out. Since the authors in the null results were assigned randomly, we would not expect the papers to group naturally by author. This likely indicates that too many principal components are being used. LDA will have more directions to choose from in high dimensional space that will allow it to spread out the groups regardless of the true relationship among the groups. However, we can still distinguish the true data because it is still more spread out than the null results. In the example with 50 principal components, we still the null results perhaps starting to group, but it looks more similar to the example with 40 principal components.** 


```{r}
# LDA on modified text data 
# topic related words removed 
p <- prcomp(y2)
w <- as.matrix(y2) %*% p$rotation[,1:60] 
l <- lda(w, grouping=b)
```


```{r}
# example with 40 principal components 
par(mfrow = c(3, 3))
answer <- sample(1:9, 1)
for (i in 1:9) {
  if (i == answer) {
    # LDA for the original data
    plot(as.matrix(w) %*% l$scaling[, 1:2], col=b)
  } else {
    # LDA with labels reassigned at random
    B <- sample(b, length(b))
    L <- lda(w, grouping=B)
    plot(as.matrix(w) %*% L$scaling[, 1:2], col=B,pch=16)
  }
}
par(mfrow = c(1, 1))
```

```{r}
# example with 60 principal components 
par(mfrow = c(3, 3))
answer <- sample(1:9, 1)
for (i in 1:9) {
  if (i == answer) {
    # LDA for the original data
    plot(as.matrix(w) %*% l$scaling[, 1:2], col=b)
  } else {
    # LDA with labels reassigned at random
    B <- sample(b, length(b))
    L <- lda(w, grouping=B)
    plot(as.matrix(w) %*% L$scaling[, 1:2], col=B,pch=16)
  }
}
par(mfrow = c(1, 1))
```
**Here, we see clear separation again. These plots (at least initially) imply that the author is likely related to word frequencies. In the example with 40 principal components, the actual data is very noticeably separated, whereas in the null results we can just barely make out vague groupings. In the example with 60 principal components, we again start to see more separation in the null results, but not nearly as extreme as we saw when LDA was run on the less modified text above. This analysis seems to imply that topic related words aren't necessarily important in distinguishing authors; even when they are not included, there appears to be a relationship between author and word frequency. **

```{r}
# LDA on modified text data 
# topic related words removed 
p <- prcomp(y3)
w <- as.matrix(y3) %*% p$rotation[,1:40] 
l <- lda(w, grouping=b)
```

```{r}
# example with 40 principal components 
par(mfrow = c(3, 3))
answer <- sample(1:9, 1)
for (i in 1:9) {
  if (i == answer) {
    # LDA for the original data
    plot(as.matrix(w) %*% l$scaling[, 1:2], col=b)
  } else {
    # LDA with labels reassigned at random
    B <- sample(b, length(b))
    L <- lda(w, grouping=B)
    plot(as.matrix(w) %*% L$scaling[, 1:2], col=B,pch=16)
  }
}
par(mfrow = c(1, 1))
```
**When LDA is performed on the text data where stop words and numbers have been removed and capitalization changed, we can still discern the true data from the null results, but the plot of the true data is much more similar to the null results than in the examples above. This indicates that perhaps capitalization and stop words are significant in distinguishing authors, but more analysis will have to be conducted.**


# Initial Cross-Validation: 

```{r}
cross_validation <- function(trials, n, y_temp){
  error <- rep(NA,trials) 
  for (i in 1:trials){
    leave <- sample(1:nrow(y_temp),5) # always take five samples in validation set
    train <- y_temp[-leave,]
    b.train <- b[-leave]
  
    # run PCA then LDA 
    p <- prcomp(train)
    l <- lda(as.matrix(train) %*% p$rotation[, 1:n], grouping=b.train)
  
    preds <- predict(l,as.matrix(y_temp[leave, ]) %*% p$rotation[, 1:n])$class
    b.test <- b[leave]
  
    error[i] <- mean(abs((b.test != preds)))
  }
  
  return(sum(error)/trials)
  
}
  
```


```{r,eval=FALSE}
avg_error_y <- cross_validation(100,40,y)
avg_error_y 
avg_error_y2 <- cross_validation(100,40,y2)
avg_error_y2
avg_error_y3 <- cross_validation(100,40,y3)
avg_error_y3
```
**Results from several rounds of cross-validation trials**

[1] 0.006
[1] 0.02
[1] 0.13

[1] 0.006
[1] 0.026
[1] 0.124

[1] 0.012
[1] 0.006
[1] 0.122

```{r,eval=FALSE}
avg_error_y50 <- cross_validation(100,50,y)
avg_error_y50 
avg_error_y2_50 <- cross_validation(100,50,y2)
avg_error_y2_50
avg_error_y3_50 <- cross_validation(100,50,y3)
avg_error_y3_50
```
[1] 0.016
[1] 0.018
[1] 0.156

[1] 0.014
[1] 0.01
[1] 0.15

[1] 0.018
[1] 0.018
[1] 0.146

```{r,eval=FALSE}
avg_error_y60 <- cross_validation(100,60,y)
avg_error_y60 
avg_error_y2_60 <- cross_validation(100,60,y2)
avg_error_y2_60
avg_error_y3_60 <- cross_validation(100,60,y3)
avg_error_y3_60
```
[1] 0.084
[1] 0.104
[1] 0.188

**The model ran on text where stop words were removed and the text was converted to lowercase appears to consistently perform the worst. This likely implies that stop words and capitalization are important to distinguishing the authors. However, the model trained on the data with the least text processing performs similarly to the model trained on the text where topic related words were removed. A comprehensive list of topic-related words was not used, which may partially explain this lack of difference. However, it may also imply that the frequency of topic related words is not very useful in distinguishing the authors. Overall, the performance of the model trained using 40 principal components is similar to the performance of the model trained using 50 principal components. Since many of the models performed similarly, I will continue to experiment will both 40 and 50 principal component vectors and these latter two forms of text processing.**


### LDA (using PCA with scaling):

```{r}
# PCA with scaling: 
absent <- which(apply(y, 2, var)==0) # which variables have 0 variance in y
z <- y[, -absent] 
p2 <- prcomp(z, scale=TRUE)

w2 <- as.matrix(z) %*% p2$rotation[,1:40] 
l2 <- lda(w2, grouping=b)
```


```{r}
# 40 principal components 
par(mfrow = c(3, 3))
answer <- sample(1:9, 1)
for (i in 1:9) {
  if (i == answer) {
    # LDA for the original data
    plot(as.matrix(w2) %*% l2$scaling[, 1:2], col=b)
  } else {
    # LDA with labels reassigned at random
    B <- sample(b, length(b))
    L <- lda(w2, grouping=B)
    plot(as.matrix(w2) %*% L$scaling[, 1:2], col=B,pch = 16)
  }
}
par(mfrow = c(1, 1))
```

```{r}
# 60 principal components
par(mfrow = c(3, 3))
answer <- sample(1:9, 1)
for (i in 1:9) {
  if (i == answer) {
    # LDA for the original data
    plot(as.matrix(w2) %*% l2$scaling[, 1:2], col=b)
  } else {
    # LDA with labels reassigned at random
    B <- sample(b, length(b))
    L <- lda(w2, grouping=B)
    plot(as.matrix(w2) %*% L$scaling[, 1:2], col=B,pch = 16)
  }
}
par(mfrow = c(1, 1))
```

**Performing PCA with scaling before running LDA initially seems to produce similar results as PCA without scaling. In both arrays of plots shown above, the true data can be relatively easily distinguished from the null results. However, it is noticeable that in the example where 60 principal components were used, the null results are not as spread out by group as was the case when PCA was done without scaling. It is possible that PCA with scaling will perform better in higher dimensions that PCA without scaling.**


```{r}
# PCA with scaling: 
absent <- which(apply(y2, 2, var)==0) # which variables have 0 variance in y
z2 <- y2[, -absent] 
p2 <- prcomp(z2, scale=TRUE)

w2 <- as.matrix(z2) %*% p2$rotation[,1:60] 
l2 <- lda(w2, grouping=b)
```

```{r}
# 40 principal components 
par(mfrow = c(3, 3))
answer <- sample(1:9, 1)
for (i in 1:9) {
  if (i == answer) {
    # LDA for the original data
    plot(as.matrix(w2) %*% l2$scaling[, 1:2], col=b)
  } else {
    # LDA with labels reassigned at random
    B <- sample(b, length(b))
    L <- lda(w2, grouping=B)
    plot(as.matrix(w2) %*% L$scaling[, 1:2], col=B,pch = 16)
  }
}
par(mfrow = c(1, 1))
```

```{r}
# 60 principal components 
par(mfrow = c(3, 3))
answer <- sample(1:9, 1)
for (i in 1:9) {
  if (i == answer) {
    # LDA for the original data
    plot(as.matrix(w2) %*% l2$scaling[, 1:2], col=b)
  } else {
    # LDA with labels reassigned at random
    B <- sample(b, length(b))
    L <- lda(w2, grouping=B)
    plot(as.matrix(w2) %*% L$scaling[, 1:2], col=B,pch = 16)
  }
}
par(mfrow = c(1, 1))
```
**We have similar results when PCA with scaling then LDA is run on the data where topic-related words were removed, as expected. Again, we see that as the number of principal components increases, our data becomes more and more spread out. However, we also see the null results begin to cluster.**

```{r}
# PCA with scaling: 
absent <- which(apply(y3, 2, var)==0) 
z3 <- y3[, -absent] 
p2 <- prcomp(z3, scale=TRUE)

w2 <- as.matrix(z3) %*% p2$rotation[,1:40] 
l2 <- lda(w2, grouping=b)
```

```{r}
# Make a null lineup to compare observed separation of the documents with null results
par(mfrow = c(3, 3))
answer <- sample(1:9, 1)
for (i in 1:9) {
  if (i == answer) {
    # LDA for the original data
    plot(as.matrix(w2) %*% l2$scaling[, 1:2], col=b)
  } else {
    # LDA with labels reassigned at random
    B <- sample(b, length(b))
    L <- lda(w2, grouping=B)
    plot(as.matrix(w2) %*% L$scaling[, 1:2], col=B,pch = 16)
  }
}
par(mfrow = c(1, 1))
```
**The grouping for the text data where stop words were removed and capitalization was changed is less spread out and harder to distinguish compared to the null results.**

# Initial Cross Validation: 

```{r}
# scale data ahead of time 
absent <- which(apply(y, 2, var)==0) 
y_temp1 <- y[, -absent] 
z <- scale(y_temp1)

absent2 <- which(apply(y2, 2, var)==0) 
y_temp2 <- y2[, -absent2] 
z2 <- scale(y_temp2)

absent3 <- which(apply(y3, 2, var)==0) 
y_temp3 <- y3[, -absent3] 
z3 <- scale(y_temp3)
```


```{r}
avg_error_z <- cross_validation(100,40,z)
avg_error_z 
avg_error_z2 <- cross_validation(100,40,z2)
avg_error_z2
avg_error_z3 <- cross_validation(100,40,z3)
avg_error_z3
```


```{r}
avg_error_z50 <- cross_validation(100,50,z)
avg_error_z50 
avg_error_z2_50 <- cross_validation(100,50,z2)
avg_error_z2_50
avg_error_z3_50 <- cross_validation(100,50,z3)
avg_error_z3_50
```

```{r}
avg_error_z60 <- cross_validation(100,60,z)
avg_error_z60 
avg_error_z2_60 <- cross_validation(100,60,z2)
avg_error_z2_60
avg_error_z3_60 <- cross_validation(100,60,z3)
avg_error_z3_60
```

**These initial cross validation results are similar across all three data sets and also do not vary very much as the number of principal components is increased -- noticeably it doesn't appear to decease for 60 principal components as we saw with PCA without scaling. However, the model appears to perform significantly worse when PCA is ran with scaling before running LDA. It is unclear to me why there is such a large difference between scaling and not scaling. However, based on these initial results, I will not use scaling with the models in the following analysis.**

### Cross-Validation 

```{r,eval=FALSE}
N = 1000 # 1000 cross validation trials
n = 40 

error <- rep(NA,N)
for (i in 1:N){
  leave <- sample(1:nrow(y),5)
  train <- y[-leave,]
  b.train <- b[-leave]
  
  # run PCA then LDA 
  p <- prcomp(train)
  l <- lda(as.matrix(train) %*% p$rotation[, 1:n], grouping=b.train)
  
  preds <- predict(l,as.matrix(y[leave, ]) %*% p$rotation[, 1:n])$class
  b.test <- b[leave]
  
  error[i] <- mean(abs((b.test != preds)))
}

```

```{r,eval=FALSE}
average_error <- sum(error)/N
average_error 
```
Average error = 0.0082

**I used cross-validation in order to calculate an overall validation error for 1000 trials of cross-validation. For each trial, this method randomly samples 5 data points that are set aside as the validation set. The remainder of the federalist papers with known (and singular authors) are set aside as the training set. PCA and then LDA is performed on the training set in order to produce a classification model. This model is used to predict the authors of the five federalist papers included in the validation set. Since the true authors of the papers in the validation set are known I then calculated the percentage of misclassified documents in each trial. I finally took an average over the errors from all 1000 trials to get an estimate of the overall validation error.** 

**Training the model on the text data that was not as processed (this data just removed numbers and words that were in all of the documents such as PUBLIUS and To the People of the State of New York) yielded an overall validation error of about 0.0082 or approximately 0.8%.**

**Although this error is extremely low, it still does not make me very confident about predicting the authors of the documents whose authors are unknown. There are a few factors to take into consideration. First of all, this does not keep track of which authors are misclassified. Through experimentation, it seems that papers authored by Madison and Hamilton were often confused. This is problematic because the author of the unknown papers is thought to be either Hamilton or Madison. If our model is confusing them, it would be helpful to quantify in what way and how often. Another potential consideration is the role Jay's documents play. Since there are so few papers he authored, there is likely not enough data to get a very accurate portrayal of his word frequencies, which may inhibit the model in some ways. Furthermore, cross-validation calculates an overall misclassification average and does not take into account that for some documents it may be harder to predict the author than for other documents. Since we do not know the authors of these 11 papers, is it likely that these are hard to classify documents and the model may perform significantly worse on them. Another important consideration is that these papers of unknown authorship could possibly have been authored by both Hamilton and Madison. As shown in the plots of the papers authored by BOTH Hamilton and Madison (shown below), these points mostly fall in Madison's range of points. This could either imply that the official classification is misleading and Madison primarily wrote these papers. Or it could imply that the model is not good at detecting papers written by multiple authors. In the latter case, we would not expect the model to accurately tell us if there was an instance where both Hamilton and Madison were the authors. This is potentially problematic.**

**There are 11 federal papers with unknown authorship. If we take the average cross-validation error to be an accurate proxy, then we would expect to classify all of these documents correctly.**


# Predicting Unknown Authors: 

```{r}
# 50 principal components
p <- prcomp(y)
l <- lda(as.matrix(y) %*% p$rotation[, 1:50], grouping=b)
unknown <- which(a == "Hamilton or Madison")
wu <- as.matrix(x[unknown, ]) %*% p$rotation[, 1:50]
predict(l,wu)$posterior
preds_y1_50 <- predict(l,wu)$class

```

```{r}
# 50 principal components
p <- prcomp(y2)
l <- lda(as.matrix(y2) %*% p$rotation[, 1:50], grouping=b)
unknown <- which(a == "Hamilton or Madison")
wu <- as.matrix(x2[unknown, ]) %*% p$rotation[, 1:50]
predict(l,wu)$posterior
preds_y2_50 <- predict(l,wu)$class

```

```{r}
# 40 principal components
p <- prcomp(y)
l <- lda(as.matrix(y) %*% p$rotation[, 1:40], grouping=b)
unknown <- which(a == "Hamilton or Madison")
wu <- as.matrix(x[unknown, ]) %*% p$rotation[, 1:40]
predict(l,wu)$posterior
preds_y1_40 <- predict(l,wu)$class

```
```{r}
# 40 principal components
p <- prcomp(y2)
l <- lda(as.matrix(y2) %*% p$rotation[, 1:40], grouping=b)
unknown <- which(a == "Hamilton or Madison")
wu <- as.matrix(x2[unknown, ]) %*% p$rotation[, 1:40]
predict(l,wu)$posterior
preds_y2_40 <- predict(l,wu)$class
```
```{r}
# 60 principal components
p <- prcomp(y)
l <- lda(as.matrix(y) %*% p$rotation[, 1:60], grouping=b)
unknown <- which(a == "Hamilton or Madison")
wu <- as.matrix(x[unknown, ]) %*% p$rotation[, 1:60]
predict(l,wu)$posterior
preds_y1_60 <- predict(l,wu)$class
```
**As shown for nearly every model, the posterior probabilities are very high (close to 1), which gives us quantitative justification to assume that the predictions made by the models are accurate -- they are made with high certainty, and the models have very low cross-validation errors**

```{r}
# 60 principal components
p <- prcomp(y2)
l <- lda(as.matrix(y2) %*% p$rotation[, 1:60], grouping=b)
unknown <- which(a == "Hamilton or Madison")
wu <- as.matrix(x2[unknown, ]) %*% p$rotation[, 1:60]
#predict(l,wu)$posterior
preds_y2_60 <- predict(l,wu)$class
```

```{r}
sum(preds_y1_50 != preds_y2_50)
which(preds_y1_40 != preds_y1_50)
which(preds_y1_40 != preds_y2_40)
which(preds_y1_60 != preds_y2_60)
sum(which(preds_y1_60 != preds_y1_40))
```

```{r}
preds_y1_50[7] 
preds_y2_50[7]
preds_y1_40[7]
preds_y2_40[7]
preds_y1_60[7]
preds_y2_60[7]
```
**Using various models to predict the unknown authors, we get very consistent results for 10 of the 11 papers. However, there is disagreement among the models about whether Hamilton or Madison wrote Federalist Paper No. 55. Four of the six models I ran predicted it was Hamilton, while the other two predicted it was Madison. I displayed the posterior probabilities above in order to help quantify the certainty of the predictions. Almost all of the posterior probabilities for the predicted author are very close to 1, while the probabilities for the other authors are nearly 0. This implies that the model is confident in its predictions, and that it does not have much uncertainty about the authors. However, it is also important to note that each model has a high posterior probability for either Hamilton or Madison for Federalist Paper No. 55. In other words, although the model is telling us it has high certainty who the author is, this certainty doesn't necessarily translate to a correct prediction -- if a single author wrote the paper, then one of the models has to be wrong. An alternative, however, is that Federalist Paper 55 was written by both Hamilton and Madison, but the model is not able to predict both authors. This could potentially explain the disagreement between the models. However, in general, due to the consistency of the predictions, the high posterior probabilities that are used to determine the prediction, and the very low cross-validation error, I do feel confidently that this model correctly predicts most of the authors of the unknown papers.** 

```{r}

# 60 principal components
p <- prcomp(y)
l <- lda(as.matrix(y) %*% p$rotation[, 1:60], grouping=b)
unknown <- which(a == "Hamilton or Madison")
wu <- as.matrix(x[unknown, ]) %*% p$rotation[, 1:60]
predict(l,wu)$posterior
preds_y1_60 <- predict(l,wu)$class
```


# Two Author Examples: 
```{r}
# 40 principal components
p <- prcomp(y2)
l <- lda(as.matrix(y2) %*% p$rotation[, 1:40], grouping=b)
two_authors <- which(a == "Hamilton and Madison")
wu <- as.matrix(x2[two_authors, ]) %*% p$rotation[, 1:40]
predict(l,wu)$posterior
preds2_y1_60 <- predict(l,wu)$class
```

```{r}
# 40 principal components
p <- prcomp(y)
l <- lda(as.matrix(y) %*% p$rotation[, 1:40], grouping=b)
two_authors <- which(a == "Hamilton and Madison")
wu <- as.matrix(x[two_authors, ]) %*% p$rotation[, 1:40]
predict(l,wu)$posterior
preds2_y1_60 <- predict(l,wu)$class
```
**For all of the papers classified as being written by two authors, the models predict the author is Madison with a posterior probability of 1 -- there is high certainty that the author is Madison. I was expecting to perhaps find inconsistency in the predictions of an author for papers that have two authors. However, that is not the case. This is not very helpful in determining whether or not Federalist Paper 55 was co-authored by Hamilton and Madison.**



```{r}
p <- prcomp(y2)
l <- lda(as.matrix(y2) %*% p$rotation[, 1:50], grouping=b)
plot(as.matrix(y2) %*% p$rotation[, 1:50] %*% l$scaling, col=b,pch=16,main = "Documents by Unknown Author" ) 
legend("topright", levels(b), text.col=1:length(b),
       fill=1:length(levels(b)))
unknown <- which(a == "Hamilton or Madison")
wu <- as.matrix(x2[unknown, ]) %*% p$rotation[, 1:50]
points(wu %*% l$scaling, col=ifelse(unknown==55, "turquoise", "blue"), pch="x", cex=2)
points(wu %*% l$scaling, col=predict(l, wu)$class, cex=3)
```


```{r}
p <- prcomp(y)
l <- lda(as.matrix(y) %*% p$rotation[, 1:40], grouping=b)
plot(as.matrix(y) %*% p$rotation[, 1:40] %*% l$scaling, col=b,pch=16,main = "Documents by Unknown Author" ) 
legend("topright", levels(b), text.col=1:length(b),
       fill=1:length(levels(b)))
unknown <- which(a == "Hamilton or Madison")
wu <- as.matrix(x[unknown, ]) %*% p$rotation[, 1:40]
points(wu %*% l$scaling, col=ifelse(unknown==55, "turquoise", "blue"), pch="x", cex=2)
points(wu %*% l$scaling, col=predict(l, wu)$class, cex=3)
```

**This plot attempts to predict who the author of the unknown papers are. The color of the circle on the exterior shows the model's prediction of who authored the paper. The model predicts that Madison authored almost all of the papers, while there is inconsistency as to whether or not Madison or Hamilton authored federalist paper 55. The plot point of Federalist Paper 55 is shown in turquoise while all of the other plot points are shown in dark blue. The point for paper 55 appears to be in between the Hamilton and Madison distributions, which perhaps explains why different models classify it as being by different authors.**

```{r}
p <- prcomp(y)
l <- lda(as.matrix(y) %*% p$rotation[, 1:40], grouping=b)
plot(as.matrix(y) %*% p$rotation[, 1:40] %*% l$scaling, col=b,pch=16,main = "Documents with Two Authors") 
legend("topright", levels(b), text.col=1:length(b),
       fill=1:length(levels(b)))
two_authors <- which(a == "Hamilton and Madison")
wu <- as.matrix(x[two_authors, ]) %*% p$rotation[, 1:40]
wu1 <- as.matrix(x[55, ]) %*% p$rotation[, 1:40]

points(wu %*% l$scaling, col=5, pch="x", cex=2)
points(wu1 %*% l$scaling, col="blue", pch="x", cex=2)
points(wu %*% l$scaling, col=predict(l, wu)$class, cex=3)
points(wu1 %*% l$scaling, col=predict(l, wu1)$class, cex=3)
```
**This plot seems to show that papers that were written by Hamilton AND Madison (shown in turquoise) were primarily written by Madison. We might expect the data points for these papers to appear somewhere in between the Hamilton and Madison distributions, however, they seem to be squarely in the Madison distribution. I added the plot point federalist paper 55 to compare it to papers written by both authors. However, it is not any closer to these data points than it is to papers written by just one author. The interpretation of this data point is difficult. Perhaps this is a paper written by both Hamilton and Madison, but Madison had more contributions that in the other co-authored papers.** 


```{r}
set.seed(361)
# Make plots showing how this method does on 5 test points
leave <- sample(1:nrow(y), 5) # pick observations to leave out 
# then run pca and lda 
train <- y[-leave, ]
b.train <- b[-leave]
p <- prcomp(train)
l <- lda(as.matrix(train) %*% p$rotation[, 1:35], grouping=b.train)
M <- p$rotation[, 1:35] %*% l$scaling[, 1:2]
train.proj <- as.matrix(train) %*% M

# predict authors: 
plot(train.proj, col=b.train,pch=16,main = "Documents by Author")
test.proj <- as.matrix(y[leave, ]) %*% M
points(test.proj, col=5, cex=2, pch="x")
legend("topright", levels(b.train), text.col=1:length(b.train),
       fill=1:length(levels(b.train)))
points(test.proj, col=predict(l, as.matrix(y[leave, ]) %*% p$rotation[, 1:35])$class, cex=3)

# colors are actual colors for actual authors: 
plot(train.proj, col=b.train,pch=16,main = "Documents by Author w/ CV Examples")
points(test.proj, col=b[leave], cex=2, pch="x")
points(test.proj, col=predict(l, as.matrix(y[leave, ]) %*% p$rotation[, 1:35])$class, cex=3)
legend("topright", levels(b.train), text.col=1:length(b.train),
       fill=1:length(levels(b.train)))
```
**This is a plot demonstrating one trial of cross validation. The data points in the validation set are signified by the X's surrounded by a circle. The color of the X corresponds to the true author, while the color of the outside circle corresponds to the predicted author. In the example above, four of the five authors were predicted correctly, but a paper written by Hamilton was misclassified as one written by Madison.**

